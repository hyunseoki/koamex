{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models.detection import keypointrcnn_resnet50_fpn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'torchvision::nms' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'torchvision::nms' is only available for these backends: [CPU, QuantizedCPU, BackendSelect, Python, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, Tracer, AutocastCPU, Autocast, Batched, VmapMode, Functionalize].\n\nCPU: registered at C:\\Users\\circleci\\project\\torchvision\\csrc\\ops\\cpu\\nms_kernel.cpp:112 [kernel]\nQuantizedCPU: registered at C:\\Users\\circleci\\project\\torchvision\\csrc\\ops\\quantized\\cpu\\qnms_kernel.cpp:125 [kernel]\nBackendSelect: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:35 [backend fallback]\nAutogradCPU: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:39 [backend fallback]\nAutogradCUDA: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:47 [backend fallback]\nAutogradXLA: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:51 [backend fallback]\nAutogradLazy: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:55 [backend fallback]\nAutogradXPU: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:43 [backend fallback]\nAutogradMLC: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:59 [backend fallback]\nAutogradHPU: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:68 [backend fallback]\nTracer: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\TraceTypeManual.cpp:293 [backend fallback]\nAutocastCPU: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\autocast_mode.cpp:461 [backend fallback]\nAutocast: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\autocast_mode.cpp:305 [backend fallback]\nBatched: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\BatchingRegistrations.cpp:1059 [backend fallback]\nVmapMode: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\nFunctionalize: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\FunctionalizeFallbackKernel.cpp:52 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\bed1\\src\\koamex\\notebook\\get_model.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/bed1/src/koamex/notebook/get_model.ipynb#ch0000001?line=2'>3</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/bed1/src/koamex/notebook/get_model.ipynb#ch0000001?line=4'>5</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(size\u001b[39m=\u001b[39m(\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m32\u001b[39m, \u001b[39m32\u001b[39m), device\u001b[39m=\u001b[39mDEVICE)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/bed1/src/koamex/notebook/get_model.ipynb#ch0000001?line=5'>6</a>\u001b[0m preds \u001b[39m=\u001b[39m model(x)\n",
      "File \u001b[1;32mc:\\Users\\bed1\\miniconda3\\envs\\hyunseoki\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\bed1\\miniconda3\\envs\\hyunseoki\\lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:98\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torchvision/models/detection/generalized_rcnn.py?line=95'>96</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(features, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m     <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torchvision/models/detection/generalized_rcnn.py?line=96'>97</a>\u001b[0m     features \u001b[39m=\u001b[39m OrderedDict([(\u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m, features)])\n\u001b[1;32m---> <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torchvision/models/detection/generalized_rcnn.py?line=97'>98</a>\u001b[0m proposals, proposal_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrpn(images, features, targets)\n\u001b[0;32m     <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torchvision/models/detection/generalized_rcnn.py?line=98'>99</a>\u001b[0m detections, detector_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroi_heads(features, proposals, images\u001b[39m.\u001b[39mimage_sizes, targets)\n\u001b[0;32m    <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torchvision/models/detection/generalized_rcnn.py?line=99'>100</a>\u001b[0m detections \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform\u001b[39m.\u001b[39mpostprocess(detections, images\u001b[39m.\u001b[39mimage_sizes, original_image_sizes)  \u001b[39m# type: ignore[operator]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bed1\\miniconda3\\envs\\hyunseoki\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\bed1\\miniconda3\\envs\\hyunseoki\\lib\\site-packages\\torchvision\\models\\detection\\rpn.py:353\u001b[0m, in \u001b[0;36mRegionProposalNetwork.forward\u001b[1;34m(self, images, features, targets)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torchvision/models/detection/rpn.py?line=350'>351</a>\u001b[0m proposals \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbox_coder\u001b[39m.\u001b[39mdecode(pred_bbox_deltas\u001b[39m.\u001b[39mdetach(), anchors)\n\u001b[0;32m    <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torchvision/models/detection/rpn.py?line=351'>352</a>\u001b[0m proposals \u001b[39m=\u001b[39m proposals\u001b[39m.\u001b[39mview(num_images, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m4\u001b[39m)\n\u001b[1;32m--> <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torchvision/models/detection/rpn.py?line=352'>353</a>\u001b[0m boxes, scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilter_proposals(proposals, objectness, images\u001b[39m.\u001b[39;49mimage_sizes, num_anchors_per_level)\n\u001b[0;32m    <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torchvision/models/detection/rpn.py?line=354'>355</a>\u001b[0m losses \u001b[39m=\u001b[39m {}\n\u001b[0;32m    <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torchvision/models/detection/rpn.py?line=355'>356</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n",
      "File \u001b[1;32mc:\\Users\\bed1\\miniconda3\\envs\\hyunseoki\\lib\\site-packages\\torchvision\\models\\detection\\rpn.py:266\u001b[0m, in \u001b[0;36mRegionProposalNetwork.filter_proposals\u001b[1;34m(self, proposals, objectness, image_shapes, num_anchors_per_level)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torchvision/models/detection/rpn.py?line=262'>263</a>\u001b[0m boxes, scores, lvl \u001b[39m=\u001b[39m boxes[keep], scores[keep], lvl[keep]\n\u001b[0;32m    <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torchvision/models/detection/rpn.py?line=264'>265</a>\u001b[0m \u001b[39m# non-maximum suppression, independently done per level\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torchvision/models/detection/rpn.py?line=265'>266</a>\u001b[0m keep \u001b[39m=\u001b[39m box_ops\u001b[39m.\u001b[39;49mbatched_nms(boxes, scores, lvl, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnms_thresh)\n\u001b[0;32m    <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torchvision/models/detection/rpn.py?line=267'>268</a>\u001b[0m \u001b[39m# keep only topk scoring predictions\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torchvision/models/detection/rpn.py?line=268'>269</a>\u001b[0m keep \u001b[39m=\u001b[39m keep[: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_nms_top_n()]\n",
      "File \u001b[1;32mc:\\Users\\bed1\\miniconda3\\envs\\hyunseoki\\lib\\site-packages\\torchvision\\ops\\boxes.py:74\u001b[0m, in \u001b[0;36mbatched_nms\u001b[1;34m(boxes, scores, idxs, iou_threshold)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torchvision/ops/boxes.py?line=71'>72</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _batched_nms_vanilla(boxes, scores, idxs, iou_threshold)\n\u001b[0;32m     <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torchvision/ops/boxes.py?line=72'>73</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torchvision/ops/boxes.py?line=73'>74</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _batched_nms_coordinate_trick(boxes, scores, idxs, iou_threshold)\n",
      "File \u001b[1;32mc:\\Users\\bed1\\miniconda3\\envs\\hyunseoki\\lib\\site-packages\\torch\\jit\\_trace.py:1118\u001b[0m, in \u001b[0;36m_script_if_tracing.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torch/jit/_trace.py?line=1113'>1114</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(fn)\n\u001b[0;32m   <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torch/jit/_trace.py?line=1114'>1115</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torch/jit/_trace.py?line=1115'>1116</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tracing():\n\u001b[0;32m   <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torch/jit/_trace.py?line=1116'>1117</a>\u001b[0m         \u001b[39m# Not tracing, don't do anything\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torch/jit/_trace.py?line=1117'>1118</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torch/jit/_trace.py?line=1119'>1120</a>\u001b[0m     compiled_fn \u001b[39m=\u001b[39m script(wrapper\u001b[39m.\u001b[39m__original_fn)  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torch/jit/_trace.py?line=1120'>1121</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m compiled_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bed1\\miniconda3\\envs\\hyunseoki\\lib\\site-packages\\torchvision\\ops\\boxes.py:93\u001b[0m, in \u001b[0;36m_batched_nms_coordinate_trick\u001b[1;34m(boxes, scores, idxs, iou_threshold)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torchvision/ops/boxes.py?line=90'>91</a>\u001b[0m offsets \u001b[39m=\u001b[39m idxs\u001b[39m.\u001b[39mto(boxes) \u001b[39m*\u001b[39m (max_coordinate \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(boxes))\n\u001b[0;32m     <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torchvision/ops/boxes.py?line=91'>92</a>\u001b[0m boxes_for_nms \u001b[39m=\u001b[39m boxes \u001b[39m+\u001b[39m offsets[:, \u001b[39mNone\u001b[39;00m]\n\u001b[1;32m---> <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torchvision/ops/boxes.py?line=92'>93</a>\u001b[0m keep \u001b[39m=\u001b[39m nms(boxes_for_nms, scores, iou_threshold)\n\u001b[0;32m     <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torchvision/ops/boxes.py?line=93'>94</a>\u001b[0m \u001b[39mreturn\u001b[39;00m keep\n",
      "File \u001b[1;32mc:\\Users\\bed1\\miniconda3\\envs\\hyunseoki\\lib\\site-packages\\torchvision\\ops\\boxes.py:40\u001b[0m, in \u001b[0;36mnms\u001b[1;34m(boxes, scores, iou_threshold)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torchvision/ops/boxes.py?line=37'>38</a>\u001b[0m     _log_api_usage_once(nms)\n\u001b[0;32m     <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torchvision/ops/boxes.py?line=38'>39</a>\u001b[0m _assert_has_ops()\n\u001b[1;32m---> <a href='file:///c%3A/Users/bed1/miniconda3/envs/hyunseoki/lib/site-packages/torchvision/ops/boxes.py?line=39'>40</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mtorchvision\u001b[39m.\u001b[39;49mnms(boxes, scores, iou_threshold)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Could not run 'torchvision::nms' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'torchvision::nms' is only available for these backends: [CPU, QuantizedCPU, BackendSelect, Python, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, Tracer, AutocastCPU, Autocast, Batched, VmapMode, Functionalize].\n\nCPU: registered at C:\\Users\\circleci\\project\\torchvision\\csrc\\ops\\cpu\\nms_kernel.cpp:112 [kernel]\nQuantizedCPU: registered at C:\\Users\\circleci\\project\\torchvision\\csrc\\ops\\quantized\\cpu\\qnms_kernel.cpp:125 [kernel]\nBackendSelect: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:35 [backend fallback]\nAutogradCPU: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:39 [backend fallback]\nAutogradCUDA: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:47 [backend fallback]\nAutogradXLA: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:51 [backend fallback]\nAutogradLazy: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:55 [backend fallback]\nAutogradXPU: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:43 [backend fallback]\nAutogradMLC: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:59 [backend fallback]\nAutogradHPU: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:68 [backend fallback]\nTracer: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\TraceTypeManual.cpp:293 [backend fallback]\nAutocastCPU: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\autocast_mode.cpp:461 [backend fallback]\nAutocast: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\autocast_mode.cpp:305 [backend fallback]\nBatched: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\BatchingRegistrations.cpp:1059 [backend fallback]\nVmapMode: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\nFunctionalize: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\FunctionalizeFallbackKernel.cpp:52 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "model = keypointrcnn_resnet50_fpn(pretrained=True, progress=False)\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "x = torch.randn(size=(2, 3, 32, 32), device=DEVICE)\n",
    "preds = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\bed1\\src\\koamex\\notebook\\get_model.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/bed1/src/koamex/notebook/get_model.ipynb#ch0000002?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(preds\u001b[39m.\u001b[39;49mshape)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print(preds.shape)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a71bb81dad70035d92ea5e7f6bc022516044efb8ebc8b8c06966443cfa9ec4cb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('hyunseoki')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
